# @package _global_

# to execute this experiment run:
# python run.py experiment=example_full.yaml

defaults:
  - override /trainer: null # override trainer to null so it's not loaded from main config defaults...
  - override /model: null
  - override /datamodule: cifar100
  - override /callbacks: null
  - override /logger: default
  - override /testmodules: cifar100

# we override default configurations with nulls to prevent them from loading at all
# instead we define all modules and their paths directly in this config,
# so everything is stored in one place

seed: 12345
deterministic: True
n_classes: 100

trainer:
  _target_: pytorch_lightning.Trainer
  gpus: 0
  min_epochs: 100
  max_epochs: 100
  weights_summary: null

model:
  _target_: src.models.SoftMax
  n_classes: ${n_classes}
  optimizer:
    _target_: torch.optim.Adam
    lr: 0.001
    weight_decay: 0.00005
  scheduler:
    _target_: torch.optim.lr_scheduler.CosineAnnealingWarmRestarts
    T_0: 100
  backbone:
    _target_: src.models.modules.wrn.WideResNet
    num_classes: ${n_classes}
    widen_factor: 2
    depth: 40
    drop_rate: 0.3

callbacks:
#  model_checkpoint:
#    _target_: pytorch_lightning.callbacks.ModelCheckpoint
#    monitor: "Accuracy/val" # name of the logged metric which determines when model is improving
#    save_top_k: 1 # save k best models (determined by above metric)
#    save_last: True # additionaly always save model from last epoch
#    mode: "max" # can be "max" or "min"
#    verbose: False
#    dirpath: "checkpoints/"
#    filename: "{epoch:02d}"
  lr_monitor:
    _target_: pytorch_lightning.callbacks.LearningRateMonitor

  softmax:
    _target_: src.callbacks.softmax.SoftmaxThresholding
    use_in_val: True
    use_in_test: True
  energy:
    _target_: src.callbacks.energy.EnergyBased
    temperature: 1
    use_in_val: True
    use_in_test: True
  mcd:
    _target_: src.callbacks.mcd.MonteCarloDropout
    num_classes: ${n_classes}
    rounds: 30
    use_in_val: False
    use_in_test: True
#  openmax:
#    _target_: src.callbacks.openmax.OpenMax
#    tailsize: 3
#    alpha: 3
#    euclid_weight: 0
#    use_in_val: False
#    use_in_test: True
  odin:
    _target_: src.callbacks.odin.ODIN
    eps: 0.02
    t: 1000
    use_in_val: False
    use_in_test: True
  tscaling:
    _target_: src.callbacks.tscaling.TemperatureScaling
    temperature: 1000
    use_in_val: True
    use_in_test: True