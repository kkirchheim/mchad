# @package _global_

defaults:
  - override /trainer: null
  - override /model: null
  - override /datamodule: cifar100-oe
  - override /callbacks: null
  - override /logger: default
  - override /testmodules: cifar100

seed: 12345
deterministic: False # TODO: not all operations are implemented as deterministic
debug: True
ignore_warnings: True
epochs: 10

trainer:
  _target_: pytorch_lightning.Trainer
  gpus: 0
  min_epochs: ${epochs}
  max_epochs: ${epochs}
  limit_train_batches: 784 # 392 batches x2
  limit_val_batches: 80 # 392 batches x2
  weights_summary: null

model:
  _target_: src.models.GCAC
  margin: 1.0
  magnitude: 1.0
  weight_center: 1.0
  weight_oe: 0.0005
  weight_ce: 1.5
  n_classes: 100
  optimizer:
    _target_: torch.optim.Adam
    lr: 0.001
    weight_decay: 0.00005
  scheduler:
    scheduler:
      _target_: torch.optim.lr_scheduler.CosineAnnealingWarmRestarts
      T_0: 3920 # 392 * epochs ${epochs}
    interval: step
    frequency: 1
  backbone:
    _target_: src.models.modules.wrn.WideResNet
    num_classes: 100
    widen_factor: 2
    depth: 40
    drop_rate: 0.3

callbacks:
  lr_monitor:
    _target_: pytorch_lightning.callbacks.LearningRateMonitor
  distance:
    _target_: src.callbacks.distance.DistanceThresholding
    use_in_val: True
    use_in_test: True
  cac:
    _target_: src.callbacks.cac.CACScorer
    use_in_val: True
    use_in_test: True
