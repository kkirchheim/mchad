# @package _global_

defaults:
  - override /trainer: null # override trainer to null so it's not loaded from main config defaults...
  - override /model: null
  - override /datamodule: svhn
  - override /callbacks: null
  - override /logger: default
  - override /testmodules: svhn

# we override default configurations with nulls to prevent them from loading at all
# instead we define all modules and their paths directly in this config,
# so everything is stored in one place

seed: 12345

# TODO: not all operations are implemented as deterministic
deterministic: False

trainer:
  _target_: pytorch_lightning.Trainer
  gpus: 0
  min_epochs: 20
  max_epochs: 20
  # limit_train_batches: 400
  #  gradient_clip_val: 0.5
  #  accumulate_grad_batches: 2
  weights_summary: null
  # resume_from_checkpoint: ${work_dir}/last.ckpt

model:
  _target_: src.models.CAC
  lr: 0.001
  weight_decay: 0.00005
  # pretrained weights?
  pretrained: null
  weight_anchor: 1.0
  magnitude: 1.0
  n_classes: 10
  backbone:
    # _target_: torchvision.models.resnet.wide_resnet101_2
    _target_: src.models.modules.wrn.WideResNet
    num_classes: 10
    widen_factor: 2
    depth: 40
    drop_rate: 0.3

callbacks:
  model_checkpoint:
    _target_: pytorch_lightning.callbacks.ModelCheckpoint
    monitor: "Accuracy/val" # name of the logged metric which determines when model is improving
    save_top_k: 1 # save k best models (determined by above metric)
    save_last: True # additionaly always save model from last epoch
    mode: "max" # can be "max" or "min"
    verbose: False
    dirpath: "checkpoints/"
    filename: "{epoch:02d}"
  lr_monitor:
    _target_: pytorch_lightning.callbacks.LearningRateMonitor

  distance:
    _target_: src.callbacks.distance.DistanceThresholding
    use_in_val: True
    use_in_test: True
  cac:
    _target_: src.callbacks.cac.CACScorer
    use_in_val: True
    use_in_test: True
